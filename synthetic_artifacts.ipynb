{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook creates a synthetic experiment to demonstrate annotation artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load up the requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn import linear_model\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set dataset sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = 30000\n",
    "ntest = 300\n",
    "nfeats = 300\n",
    "nclasses = 3\n",
    "\n",
    "assert nfeats % nclasses == 0 # We want to make sure we have three (orthonormal) clusters. Not strictly required.\n",
    "assert ntrain % nclasses == 0 # Ensures balanced train data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred, ref):\n",
    "    correct = sum((pred == ref).astype('int'))\n",
    "    return correct/len(ref)\n",
    "\n",
    "\n",
    "def create_ex_uniform_random():\n",
    "    \"\"\"\n",
    "    Sample from uniform random distribution to create train and test sets.\n",
    "    \"\"\"\n",
    "    X = np.random.rand(ntrain, nfeats)\n",
    "    Y = np.random.randint(0, nclasses, ntrain)\n",
    "    xtest = np.random.rand(ntest, nfeats)\n",
    "    ytest = np.random.randint(0, nclasses, ntest)\n",
    "    \n",
    "    return X, Y, xtest, ytest\n",
    "\n",
    "\n",
    "def create_ex_multivariate_normal(nfeats, ntrain, ntest):\n",
    "    \"\"\"\n",
    "    Sample from 3 different multivariate Gaussian distributions to create train and test sets.\n",
    "    \"\"\"\n",
    "    covariance = np.diag(np.full(nfeats, 10))  # Use the same for all 3 clusters.\n",
    "    mean = np.full(nfeats, 0)\n",
    "    mean[nfeats // nclasses] = 1  # To ensure orthonormality of clusters.\n",
    "    \n",
    "    X = np.random.multivariate_normal(mean, covariance, ntrain//nclasses)\n",
    "    Y = np.full(ntrain//nclasses, 0)\n",
    "    xtest = np.random.multivariate_normal(mean, covariance, ntest//nclasses) \n",
    "    ytest = np.full(ntest//nclasses, 0)\n",
    "    \n",
    "    \n",
    "    for i in range(1, nclasses):\n",
    "        mean = np.full(nfeats, 0)\n",
    "        mean[(i+1) * nfeats // nclasses - 1] = 1\n",
    "    \n",
    "        ex = np.random.multivariate_normal(mean, covariance, ntrain//nclasses)\n",
    "        X = np.concatenate((X, ex), axis=0)\n",
    "        \n",
    "        label = np.full(ntrain//nclasses, i)\n",
    "        Y = np.concatenate((Y, label), axis=0)\n",
    "        \n",
    "        test_ex = np.random.multivariate_normal(mean, covariance, ntest//nclasses)\n",
    "        xtest = np.concatenate((xtest, test_ex), axis=0)\n",
    "        \n",
    "        test_y = np.full(ntest//nclasses, i)\n",
    "        ytest = np.concatenate((ytest, test_y), axis=0)\n",
    "        \n",
    "    # Shuffle the train.\n",
    "    floaty = np.expand_dims(Y, axis=1).astype('float')\n",
    "    xy = np.concatenate((X, floaty), axis=1)\n",
    "    np.random.shuffle(xy)\n",
    "    X = xy.T[:-1].T\n",
    "    Y = xy.T[-1].T.astype('int')\n",
    "    return X, Y, xtest, ytest\n",
    "\n",
    "\n",
    "def introduce_cheat_feature(x, y, randomize):\n",
    "    if randomize:\n",
    "        random_feats = int(len(x) * randomize)//3\n",
    "        mask0 = np.full(random_feats, 0)\n",
    "        mask1 = np.full(random_feats, 1)\n",
    "        mask2 = np.full(random_feats, 2)\n",
    "\n",
    "        mask = np.concatenate((mask0, mask1, mask2))\n",
    "        np.random.shuffle(mask)\n",
    "        \n",
    "        masked_y = np.copy(y)\n",
    "        masked_y[:(len(mask))] = mask\n",
    "    else:\n",
    "        masked_y = y\n",
    "\n",
    "    ycheatfeat = np.expand_dims(masked_y.astype('float'), axis=1)\n",
    "    xcheatfeat = np.concatenate((x, ycheatfeat), axis=1)\n",
    "    return xcheatfeat\n",
    "\n",
    "# introduce_cheat_feature(np.random.rand(18, 3), np.random.randint(0, 3, 18), 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: 30000, Y:30000, x-test:300, y-test:300\n"
     ]
    }
   ],
   "source": [
    "# X, Y, xtest, ytest = create_ex_uniform_random()\n",
    "\n",
    "X, Y, xtest, ytest = create_ex_multivariate_normal(nfeats, ntrain, ntest)\n",
    "print(f\"X: {len(X)}, Y:{len(Y)}, x-test:{len(xtest)}, y-test:{len(ytest)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "#### Train and test a linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.exists('baseline.joblib'):\n",
    "    clf = load('baseline.joblib')\n",
    "else:\n",
    "    clf = linear_model.SGDClassifier(max_iter=1000, tol=1e-3)\n",
    "    clf.fit(X, Y)\n",
    "    dump(clf, 'baseline.joblib') \n",
    "\n",
    "ypred = clf.predict(xtest)\n",
    "accuracy(ypred, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cheating\n",
    "#### Train a cheater model, which uses labels as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "cheat_train = introduce_cheat_feature(X, Y, randomize=0.)\n",
    "\n",
    "if os.path.exists('cheater.joblib'):\n",
    "    cheater = load('cheater.joblib')\n",
    "else:\n",
    "    cheater = linear_model.SGDClassifier(max_iter=1000, tol=1e-3)\n",
    "    cheater.fit(cheat_train, Y)\n",
    "    dump(cheater, 'cheater.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### At various levels of randomization of the cheat feature, see test performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 0.0 randomization, acc=0.99\n",
      "At 0.2 randomization, acc=0.8567\n",
      "At 0.4 randomization, acc=0.74\n",
      "At 0.6 randomization, acc=0.65\n",
      "At 0.8 randomization, acc=0.5133\n",
      "At 0.9 randomization, acc=0.4233\n",
      "At 1.0 randomization, acc=0.3367\n"
     ]
    }
   ],
   "source": [
    "for r in [0., 0.2, 0.4, 0.6, 0.8, 0.9, 1.]:\n",
    "    cheat_test = introduce_cheat_feature(xtest, ytest, randomize=r)\n",
    "\n",
    "    cheat_prediction = cheater.predict(cheat_test)\n",
    "    print(f\"At {r} randomization, acc={accuracy(cheat_prediction, ytest):.4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cheating some of the time\n",
    "#### This model cheats some of the time; 50% of the time, cheat features are identical to the true class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_cheat_train = introduce_cheat_feature(X, Y, randomize=.5)\n",
    "\n",
    "if os.path.exists('partial_cheater.joblib'):\n",
    "    partial_cheater = load('partial_cheater.joblib')\n",
    "else:\n",
    "    partial_cheater = linear_model.SGDClassifier(max_iter=1000, tol=1e-3)\n",
    "    partial_cheater.fit(partial_cheat_train, Y)\n",
    "    dump(partial_cheater, 'partial_cheater.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### At various levels of randomization of the cheat feature, see test performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 0.0 randomization, acc=0.61\n",
      "At 0.2 randomization, acc=0.5433\n",
      "At 0.4 randomization, acc=0.5067\n",
      "At 0.6 randomization, acc=0.4933\n",
      "At 0.8 randomization, acc=0.4467\n",
      "At 0.9 randomization, acc=0.3967\n",
      "At 1.0 randomization, acc=0.36\n"
     ]
    }
   ],
   "source": [
    "for r in [0., 0.2, 0.4, 0.6, 0.8, 0.9, 1.]:\n",
    "    partial_cheat_test = introduce_cheat_feature(xtest, ytest, randomize=r)\n",
    "\n",
    "    partial_cheat_pred = partial_cheater.predict(partial_cheat_test)\n",
    "    print(f\"At {r} randomization, acc={accuracy(partial_cheat_pred, ytest):.4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
